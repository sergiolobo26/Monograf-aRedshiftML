@book{Hastie2009,
 	address = {New York, NY},
 	archivePrefix = {arXiv},
 	arxivId = {1010.3003},
 	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
 	booktitle = {Elements of Statistical Learning},
 	doi = {10.1007/b94608},
 	eprint = {1010.3003},
 	isbn = {978-0-387-84857-0},
 	issn = {0964-1998},
 	keywords = {},
 	mendeley-groups = {TesisMIMEC},
 	pages = {},
 	pmid = {15512507},
 	publisher = {Springer New York},
 	series = {Springer Series in Statistics},
 	title = {{The Elements of Statistical Learning}},
 	url = {},
 	year = {2009}
 }
 
@book{Bishop,
	address = {Singapore},
	author = {Bishop, Christopher M.},
	edition = {1},
	file = {:C$\backslash$:/Users/Sergio/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bishop - Unknown - Pattern Recognition and Machine Learning.pdf:pdf},
	isbn = {9780387310732},
	mendeley-groups = {TesisMIMEC},
	publisher = {Springer Berlin Heidelberg},
	title = {{Pattern Recognition and Machine Learning}},
	url = {http://research.microsoft.com/âˆ¼cmbishop/PRML}
}

@misc{Vapnik2000,
	abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: the setting of learning problems based on the model of minimizing the risk functional from empirical data a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency non-asymptotic bounds for the risk achieved using the empirical risk minimization principle principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds the Support Vector methods that control the generalization ability when estimating function using small sample size.The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation a new inductive principle of learning.Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists.Vladimir N. Vapnik is Technology Leader AT{\&}T Labs-Research and Professor of London University. He is one of the founders of statistical learning theory, and the author of seven books published in English, Russian, German, and Chinese.},
	author = {Vapnik, Vladimir N},
	booktitle = {New York},
	doi = {10.1109/TNN.1997.641482},
	file = {:C$\backslash$:/Users/Sergio/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vapnik - 2000 - The nature of statistical learning theory.pdf:pdf},
	isbn = {0387987800},
	issn = {10459227},
	mendeley-groups = {TesisMIMEC},
	number = {6},
	pages = {314},
	pmid = {18255760},
	title = {{The nature of statistical learning theory}},
	url = {http://books.google.com/books?id=sna9BaxVbj8C{\&}pgis=1},
	volume = {8},
	year = {2000}
}

@article{Hofmann2008,
	abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
	archivePrefix = {arXiv},
	arxivId = {math/0701907},
	author = {Hofmann, Thomas and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J.},
	doi = {10.1214/009053607000000677},
	eprint = {0701907},
	file = {:C$\backslash$:/Users/Sergio/Desktop/kernell{\_}methods{\_}in{\_}ML.pdf:pdf},
	isbn = {0090-5364},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Graphical models,Machine learning,Reproducing kernels,Support vector machines},
	number = {3},
	pages = {1171--1220},
	primaryClass = {math},
	title = {{Kernel methods in machine learning}},
	volume = {36},
	year = {2008}
}

@article{SMOLA2004,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {SMOLA, ALEX J. and BERNHARD SCHOLKOPF},
	doi = {10.1023/B:STCO.0000035301.49549.88},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Sergio/Downloads/SmolaSchoelkopf.pdf:pdf},
	isbn = {0960-3174},
	issn = {0960-3174},
	journal = {Statistics and Computing},
	keywords = {machine learning,regression estimation,support vector machines},
	mendeley-groups = {Deep Learning {\&} and ML},
	pages = {199--222},
	pmid = {17969693},
	title = {{A tutorial on support vector regression}},
	url = {http://download.springer.com/static/pdf/493/art{\%}3A10.1023{\%}2FB{\%}3ASTCO.0000035301.49549.88.pdf?auth66=1408162706{\_}8a28764ed0fae918f8fb14e785695336{\&}ext=.pdf},
	year = {2004}
}

@webpage{krr, title={Scikit-learn 0.19.2 documentation }, author         = "Scikit Learn - Python Library"}

@article{Aghamousa:2016zmz,
	author         = "Aghamousa, Amir and others",
	title          = "{The DESI Experiment Part I: Science,Targeting, and
	Survey Design}",
	collaboration  = "DESI",
	year           = "2016",
	eprint         = "1611.00036",
	archivePrefix  = "arXiv",
	primaryClass   = "astro-ph.IM",
	reportNumber   = "FERMILAB-PUB-16-517-AE",
	SLACcitation   = "%%CITATION = ARXIV:1611.00036;%%"
}

@article{Aghamousa:2016sne,
	author         = "Aghamousa, Amir and others",
	title          = "{The DESI Experiment Part II: Instrument Design}",
	collaboration  = "DESI",
	year           = "2016",
	eprint         = "1611.00037",
	archivePrefix  = "arXiv",
	primaryClass   = "astro-ph.IM",
	reportNumber   = "FERMILAB-PUB-16-518-AE",
	SLACcitation   = "%%CITATION = ARXIV:1611.00037;%%"
}

@article{Nord:2016plv,
	author         = "Nord, Brian D. and others",
	title          = "{SPOKES: an End-to-End Simulation Facility for
	Spectroscopic Cosmological Surveys}",
	journal        = "Astron. Comput.",
	volume         = "15",
	year           = "2016",
	pages          = "1-15",
	doi            = "10.1016/j.ascom.2016.02.001",
	eprint         = "1602.01480",
	archivePrefix  = "arXiv",
	primaryClass   = "astro-ph.IM",
	reportNumber   = "FERMILAB-PUB-16-009-AE",
	SLACcitation   = "%%CITATION = ARXIV:1602.01480;%%"
}

@webpage{DESIscience,
	author         = "DESI",
	title          = "DESI Final Design Report Part I: Science, Targeting, and Survey Design",
	year           = "2016",
	url = "http://desi.lbl.gov/wp-content/uploads/2014/04/fdr-science-biblatex.pdf"
}



